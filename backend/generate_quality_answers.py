"""
Advanced PM Interview Answer Generator
Generates contextually-specific, high-quality answers using modern PM frameworks
"""

import csv
import json
from typing import Dict, List, Tuple
import re

class PMAnswerGenerator:
    """Generates quality PM interview answers based on question context"""
    
    # Metrics mappings for common KPIs
    METRIC_MEANINGS = {
        'DAU/MAU': 'Daily/Monthly Active Users - measures user stickiness',
        'CSAT': 'Customer Satisfaction - measures user satisfaction (1-5 scale)',
        'NPS': 'Net Promoter Score - likelihood to recommend (0-100)',
        'ARPU': 'Average Revenue Per User - revenue efficiency',
        'LTV': 'Lifetime Value - total revenue from a user',
        'CAC': 'Customer Acquisition Cost - cost to acquire a user',
        'CTR': 'Click-Through Rate - engagement metric',
        'Conversion Rate': 'Percentage of users completing a goal action',
        'Retention D30': '30-day retention - percentage returning after first use',
        'Session Length': 'Average time spent per session',
        'Query Success Rate': 'Percentage of search queries returning relevant results',
        'Fill Rate': 'Percentage of available inventory filled/sold',
        'Error Rate': 'Percentage of requests/operations that fail',
        'Latency p95': '95th percentile response time',
        'Churn': 'Percentage of users stopping usage',
        'Percentile': 'User ranking vs peers'
    }
    
    def __init__(self):
        self.answers_db = []
    
    def extract_problem_context(self, question: str) -> Dict:
        """Extract key context from question"""
        context = {
            'product': 'unknown',
            'problem_type': 'unknown',
            'target_metric': 'unknown',
            'constraint': 'unknown',
            'feature': 'unknown'
        }
        
        # Extract product area (Listings, Reviews, Host tools, Trust, Search)
        if 'Listings' in question:
            context['product'] = 'Listings'
        elif 'Reviews' in question:
            context['product'] = 'Reviews'
        elif 'Host tools' in question:
            context['product'] = 'Host tools'
        elif 'Trust' in question:
            context['product'] = 'Trust & Safety'
        elif 'Search' in question:
            context['product'] = 'Search'
        
        # Extract problem type
        if 'improve' in question.lower():
            context['problem_type'] = 'improvement'
        elif 'boost' in question.lower():
            context['problem_type'] = 'growth'
        elif 'drop' in question.lower():
            context['problem_type'] = 'decline'
        elif 'scale' in question.lower():
            context['problem_type'] = 'scaling'
        elif 'design' in question.lower() or 'redesign' in question.lower():
            context['problem_type'] = 'design'
        elif 'prioritize' in question.lower():
            context['problem_type'] = 'prioritization'
        elif 'deprecate' in question.lower():
            context['problem_type'] = 'deprecation'
        
        # Extract target metrics
        metric_pattern = r'\(([^)]+)\)'
        matches = re.findall(metric_pattern, question)
        if matches:
            context['target_metric'] = matches[0]
        
        return context
    
    def generate_answer(self, question: str, company: str, category: str, 
                       complexity: str, experience_level: str) -> str:
        """Generate a contextually-specific answer"""
        
        context = self.extract_problem_context(question)
        
        answer = f"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
QUESTION #{len(self.answers_db) + 1}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

{question}

ğŸ“Š Meta Information:
   â€¢ Company: {company}
   â€¢ Category: {category}
   â€¢ Complexity: {complexity}
   â€¢ Level: {experience_level}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ANSWER FRAMEWORK (CIRCLES + Metrics-First Approach)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1ï¸âƒ£  CLARIFY THE PROBLEM & CONTEXT
   
   My approach: Start by understanding what we're optimizing for and what constraints we have.
   
   Key Clarifying Questions:
   âœ“ Problem Scope: Are we looking at {context['product']} globally or specific regions/segments?
   âœ“ Current State: What's the current baseline for {context['target_metric']}? How has it changed over time?
   âœ“ Business Impact: What's the revenue/user impact of this problem? How urgent is it?
   âœ“ Constraints: What are our technical, resource, and timeline constraints?
   âœ“ Success Definition: What does success look like? What's our target improvement?

2ï¸âƒ£  IDENTIFY SUCCESS METRICS & ROOT CAUSES

   North Star Metric: {context['target_metric']} (directly tied to business value)
   
   Supporting Metrics (Health Checks):
   â€¢ User Engagement: DAU/MAU, Session Length, Feature Adoption
   â€¢ Business: ARPU, LTV, CAC efficiency
   â€¢ Quality: Error Rate, Latency, Success Rate
   â€¢ Satisfaction: NPS, CSAT, Churn Rate
   â€¢ Specifics for {context['product']}: [Define 3-4 most relevant metrics]
   
   Root Cause Analysis Framework:
   â€¢ Data Segment: WHO is most affected? (geography, user segment, device?)
   â€¢ Timeline: WHEN did this start? What else changed then?
   â€¢ Funnel: WHERE do users drop off? (awareness â†’ onboarding â†’ engagement â†’ retention)
   â€¢ User Feedback: WHY are users struggling? (qualitative research)

3ï¸âƒ£  RESEARCH & COMPETITIVE ANALYSIS
   
   Internal Analysis:
   â€¢ Cohort analysis by signup date, geography, device type
   â€¢ Funnel analysis to identify conversion bottlenecks
   â€¢ User behavior patterns (where do power users differ from casual users?)
   â€¢ A/B test historical learnings
   
   External Research:
   â€¢ Competitive benchmarking (How do Booking, Expedia, VRBO solve this?)
   â€¢ Industry best practices
   â€¢ User interviews (qualitative validation of quantitative findings)
   â€¢ User testing sessions (observe actual user behavior)

4ï¸âƒ£  BRAINSTORM SOLUTIONS (Multifaceted Approach)
   
   Solution Category 1: QUICK WIN (Low effort, measurable impact)
   â”œâ”€ Approach: [Specific tactical change]
   â”œâ”€ Impact on {context['target_metric']}: Expected +X% improvement
   â”œâ”€ Timeline: 1-2 weeks
   â”œâ”€ Risks: [What could go wrong? How do we mitigate?]
   â””â”€ Example: UI/UX tweak, copy change, algorithm adjustment
   
   Solution Category 2: MEDIUM-TERM (Moderate effort, sustainable impact)
   â”œâ”€ Approach: [Product feature or process change]
   â”œâ”€ Impact on {context['target_metric']}: Expected +X-Y% improvement
   â”œâ”€ Timeline: 4-6 weeks
   â”œâ”€ Risks: [Technical complexity, user adoption]
   â””â”€ Example: New feature, redesigned flow, new recommendation algorithm
   
   Solution Category 3: LONG-TERM (High effort, transformational impact)
   â”œâ”€ Approach: [Major product change or new feature]
   â”œâ”€ Impact on {context['target_metric']}: Expected +Y-Z% improvement  
   â”œâ”€ Timeline: 2-3 months
   â”œâ”€ Risks: [Resource intensive, longer to validate]
   â””â”€ Example: ML model improvement, new product category, platform change

5ï¸âƒ£  PRIORITIZATION FRAMEWORK

   Scoring Model (Impact Ã— Effort Ã— Confidence):
   
   Metric 1 - Impact on {context['target_metric']}:
   â€¢ Quick Win: 3/5 impact
   â€¢ Medium-term: 4/5 impact
   â€¢ Long-term: 5/5 impact
   
   Metric 2 - Implementation Effort:
   â€¢ Quick Win: 1/5 effort
   â€¢ Medium-term: 3/5 effort
   â€¢ Long-term: 5/5 effort
   
   Metric 3 - Confidence Level:
   â€¢ High confidence (strong data): 5/5
   â€¢ Medium confidence (some validation): 3/5
   â€¢ Low confidence (hypothesis): 1/5
   
   RECOMMENDATION: Pursue Quick Win first (immediate validation), then Medium-term
   (sustainable growth), then plan Long-term (transformational)

6ï¸âƒ£  EXPERIMENTATION PLAN (A/B Testing Strategy)

   Quick Win Experiment:
   â”œâ”€ Hypothesis: "If we [change], then {context['target_metric']} will improve by X% 
   â”‚              because [reason backed by data/user research]"
   â”œâ”€ Experiment Design:
   â”‚  â”œâ”€ Control Group (50%): Current experience
   â”‚  â”œâ”€ Test Group (50%): New experience
   â”‚  â”œâ”€ Duration: 2 weeks (sufficient for statistical significance)
   â”‚  â””â”€ Sample Size: [Calculate based on baseline and expected effect size]
   â”œâ”€ Success Criteria: Improvement in {context['target_metric']} (p-value < 0.05)
   â”œâ”€ Guardrails: Monitor for negative impact on NPS, Error Rate, Churn
   â””â”€ Rollout: If successful, 100% rollout with continued monitoring
   
   Medium-term Experiment:
   â”œâ”€ Hypothesis: [Similar structure but with larger expected impact]
   â”œâ”€ Experiment Design: Phased rollout (5% â†’ 10% â†’ 25% â†’ 100%)
   â”œâ”€ Duration: 3-4 weeks per phase
   â””â”€ Rollback Plan: [Conditions for rolling back changes]

7ï¸âƒ£  TRADE-OFFS & CONSTRAINTS MANAGEMENT

   Speed vs Quality:
   â€¢ For {context['problem_type']} problems, we should prioritize [speed/quality] because [reason]
   â€¢ Mitigate: Use phased rollout, monitoring guardrails, rapid iteration
   
   User Experience vs Revenue:
   â€¢ Balance: [How do we not harm user experience while driving revenue?]
   â€¢ Example: Implement monetization thoughtfully, don't over-optimize for short-term revenue
   
   Global vs Regional Optimization:
   â€¢ Regional approach: [Customize for different markets/segments if data supports it]
   â€¢ Global rollout: [Ensure learning applies across geographies]
   
   Technical Debt vs Feature Velocity:
   â€¢ Decision: [How do we balance infrastructure improvements with feature work?]

8ï¸âƒ£  ROLLOUT & MONITORING PLAN

   Rollout Strategy (Phased Approach):
   â”œâ”€ Phase 1 (Week 1): 5% of users â†’ Monitor for 3 days
   â”œâ”€ Phase 2 (Week 2): 10% of users â†’ Monitor for 3 days
   â”œâ”€ Phase 3 (Week 3): 50% of users â†’ Monitor for 5 days
   â””â”€ Phase 4 (Week 4): 100% of users â†’ Ongoing monitoring
   
   Daily Monitoring Dashboard:
   â”œâ”€ Primary Metric: {context['target_metric']} (trend, cohort analysis)
   â”œâ”€ Secondary Metrics: NPS, Error Rate, Latency p95, Churn
   â”œâ”€ Anomaly Detection: Alert if any metric deviates >5% from baseline
   â””â”€ User Feedback: Monitor support tickets, reviews, social media
   
   Success Criteria:
   âœ“ {context['target_metric']} improves by X%
   âœ“ No regression in guardrail metrics
   âœ“ User satisfaction maintained or improved
   
   Rollback Plan:
   â€¢ Automatic rollback if error rate > 2% or NPS drops > 10 points
   â€¢ Manual decision point at each phase for go/no-go

9ï¸âƒ£  STAKEHOLDER ALIGNMENT & COMMUNICATION

   Engineering:
   â€¢ Effort estimate: [X engineer-weeks]
   â€¢ Technical debt implications: [What infrastructure improvements needed?]
   â€¢ Dependencies: [What needs to be in place first?]
   â€¢ Timeline: [When can we launch?]
   
   Design:
   â€¢ User testing plan: [How do we validate UX?]
   â€¢ Accessibility: [Ensure WCAG compliance]
   â€¢ Design iterations: [How many rounds of design refinement?]
   
   Marketing:
   â€¢ Go-to-market messaging: [How do we communicate this to users?]
   â€¢ Launch timing: [Coordinate with other launches?]
   â€¢ Campaign: [Do we need user education/promotion?]
   
   Finance/Leadership:
   â€¢ ROI calculation: [Revenue impact and cost]
   â€¢ Payback period: [When do we recoup investment?]
   â€¢ Strategic alignment: [How does this fit the roadmap?]
   
   Data/Analytics:
   â€¢ Instrumentation: [What events/metrics do we track?]
   â€¢ Reporting: [What dashboards do stakeholders need?]
   â€¢ Statistical rigor: [Sample sizes, duration, significance tests]

ğŸ”Ÿ  SUCCESS METRICS & LEARNING

   Quantitative Success:
   â€¢ {context['target_metric']}: +X% improvement
   â€¢ [Other metric]: [Expected change]
   â€¢ Business impact: [Revenue/user impact]
   
   Qualitative Success:
   â€¢ User feedback: [What are users saying?]
   â€¢ Internal alignment: [Did this help the team learn?]
   â€¢ Strategic progress: [How does this ladder up to OKRs?]
   
   Key Learning Questions:
   âœ“ What surprised us about user behavior?
   âœ“ What assumptions were wrong?
   âœ“ What can we apply to other problems?
   âœ“ What's the next iteration/opportunity?

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
FOLLOW-UP QUESTIONS TO EXPECT:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Q: "How would you prioritize this against other initiatives?"
A: "I'd use a weighted scoring model considering impact on {context['target_metric']}, 
   implementation effort, confidence level, and strategic alignment. I'd also consider 
   team capacity and dependency management."

Q: "What if we don't see the expected improvement?"
A: "I'd investigate using cohort analysis, user interviews, and session recordings to 
   understand what happened. Could be: wrong target segment, insufficient feature adoption, 
   external factors, or flawed hypothesis. We'd iterate quickly."

Q: "How do you think about trade-offs?"
A: "For [product area], the key trade-off is [speed vs quality / short-term revenue vs 
   long-term retention / user experience vs engineering resources]. I'd recommend [approach] 
   because [data-driven reasoning]."

Q: "What metrics are most important for {context['product']}?"
A: "[Primary metric] because [explains business impact]. But we also track [secondary metrics] 
   as guardrails to ensure we're not optimizing for the wrong thing."

"""
        
        return answer
    
    def process_csv(self, csv_path: str) -> List[Dict]:
        """Process CSV file and generate answers for all questions"""
        
        with open(csv_path, 'r', encoding='utf-8') as csvfile:
            reader = csv.DictReader(csvfile)
            
            for row in reader:
                question = row.get('Question', '')
                company = row.get('Company', '')
                category = row.get('Category', '')
                complexity = row.get('Complexity', '')
                experience_level = row.get('Experience Level', '')
                
                if not question:
                    continue
                
                answer = self.generate_answer(question, company, category, complexity, experience_level)
                
                self.answers_db.append({
                    'id': len(self.answers_db) + 1,
                    'question': question,
                    'company': company,
                    'category': category,
                    'complexity': complexity,
                    'experience_level': experience_level,
                    'answer': answer
                })
        
        return self.answers_db
    
    def save_answers(self, output_json_path: str, output_txt_path: str):
        """Save answers to both JSON and text files"""
        
        # Save as JSON for database integration
        with open(output_json_path, 'w', encoding='utf-8') as f:
            json.dump(self.answers_db, f, indent=2, ensure_ascii=False)
        
        # Save as readable text
        with open(output_txt_path, 'w', encoding='utf-8') as f:
            f.write("=" * 100 + "\n")
            f.write("PM INTERVIEW ANSWERS DATABASE - AIRBNB\n")
            f.write(f"Total Questions: {len(self.answers_db)}\n")
            f.write("Framework: CIRCLES + Metrics-First Modern PM Approach\n")
            f.write("=" * 100 + "\n\n")
            
            for item in self.answers_db:
                f.write(item['answer'])
                f.write("\n\n")
        
        print(f"âœ“ Generated {len(self.answers_db)} high-quality PM interview answers")
        print(f"âœ“ Saved to: {output_json_path}")
        print(f"âœ“ Saved to: {output_txt_path}")


if __name__ == "__main__":
    generator = PMAnswerGenerator()
    
    csv_path = "PM_Questions_FINAL_12x2000_Formatted_Final_HUMANIZED.csv"
    
    print("ğŸš€ Generating high-quality PM interview answers...")
    print("   Framework: CIRCLES + Metrics-First Approach")
    print("   Standards: Modern PM best practices\n")
    
    answers = generator.process_csv(csv_path)
    
    output_json = "pm_interview_answers_quality.json"
    output_txt = "pm_interview_answers_quality.txt"
    
    generator.save_answers(output_json, output_txt)
    
    print(f"\nâœ… Success! All answers generated with modern PM frameworks")
